from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(title="LLM ë‰´ìŠ¤ ë¶„ë¥˜ API")

# ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜
class InferenceInput(BaseModel):
    text: str

# ë¼ë²¨ ì •ì˜ (ì¶”ë¡  ëŒ€ìƒ í´ë˜ìŠ¤ ìˆœì„œì™€ ì¼ì¹˜í•´ì•¼ í•¨)
LABELS =['education', 'human interest', 'society', 'sport', 'crime, law and justice',
'disaster, accident and emergency incident', 'arts, culture, entertainment and media', 'politics',
'economy, business and finance', 'lifestyle and leisure', 'science and technology',
'health', 'labour', 'religion', 'weather', 'environment', 'conflict, war and peace']
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_PATH = "classla/multilingual-IPTC-news-topic-classifier"  # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” HuggingFace ëª¨ë¸ëª…
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
model.eval()

# ì˜ˆì¸¡ API
@app.post("/predict")
def predict(input_data: InferenceInput):
    try:
        inputs = tokenizer(input_data.text, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1)
            pred_idx = torch.argmax(probs, dim=1).item()
            confidence = round(probs[0][pred_idx].item(), 4)

        return {
            "label": LABELS[pred_idx],
            "confidence": confidence
        }

    except Exception as e:
        logger.error(f"ğŸ”¥ Prediction error: {e}")
        raise HTTPException(status_code=500, detail="Prediction failed.")

# í—¬ìŠ¤ ì²´í¬
@app.get("/health")
def health_check():
    return {"status": "ok"}